# V-ICR: Video Iterative Context Refinement

**Video-based Action Recognition Pipeline** - Action recognition system using YOLO + ByteTrack tracking and Qwen3-VL based iterative context refinement

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

[ğŸ‡°ğŸ‡· í•œêµ­ì–´ README](../README.md)

## ğŸ“‹ Overview

V-ICR is an end-to-end pipeline that detects and tracks people in video, then recognizes each person's actions using Vision-Language Model (VLM).

### Key Features

- **ğŸ¯ Precise Person Tracking**: Robust multi-object tracking based on YOLO12 + ByteTrack
- **ğŸ”§ Track Post-processing**: Kalman filter smoothing, broken track stitching
- **ğŸ§  VLM-based Action Recognition**: Per-second action classification using Qwen3-VL-8B
- **ğŸ”„ Temporal Soft-label Refinement**: Iterative refinement using soft-label candidates and temporal context
- **ğŸ“Š Similar Action Grouping**: VLM-based automatic action categorization and labelmap generation
- **ğŸ“¦ Unified Label Output**: Frame-level bbox + action label integrated data

## ğŸ—ï¸ System Architecture

```
Input Video (MP4)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Detector       â”‚ â† YOLO12 + ByteTrack
â”‚  (Detection &     â”‚
â”‚   Tracking)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Tubes   â”‚ â† Per-person cropped videos
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Recognizer      â”‚ â† Qwen3-VL-8B
â”‚  (Action          â”‚
â”‚   Recognition)    â”‚
â”‚                   â”‚
â”‚  1. Per-second    â”‚
â”‚     analysis      â”‚
â”‚  2. Soft-label    â”‚
â”‚     top 5         â”‚
â”‚  3. Similar actionâ”‚
â”‚     grouping      â”‚
â”‚  4. Labelmap      â”‚
â”‚     refinement    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Exporter       â”‚
â”‚  (Label Data      â”‚
â”‚   Generation)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
  output/<video>.json
```

## ğŸ“ Project Structure

```
V-ICR/
â”œâ”€â”€ run.py                    # Main execution script
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ checkpoints/              # Model weights
â”‚   â””â”€â”€ yolo12x.pt           # YOLO12 weights
â”œâ”€â”€ modules/                  # Core modules
â”‚   â”œâ”€â”€ detector.py          # Detection and tracking module
â”‚   â”œâ”€â”€ recognizer.py        # Action recognition module
â”‚   â”œâ”€â”€ exporter.py          # Label data export module
â”‚   â”œâ”€â”€ dataset.py           # Dataset utilities
â”‚   â””â”€â”€ bytetrack_tuned.yaml # ByteTrack configuration
â”œâ”€â”€ utils/                    # Utilities
â”‚   â””â”€â”€ logger.py            # Logging utility
â”œâ”€â”€ data/                     # Data directory
â”‚   â”œâ”€â”€ input/               # Input videos (place MP4 files here)
â”‚   â”œâ”€â”€ working/             # Intermediate results
â”‚   â””â”€â”€ output/              # Final label data output
â””â”€â”€ docs/                     # Documentation
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Install dependencies
pip install -r requirements.txt
```

### 2. Model Preparation

Place YOLO12 weights in the `checkpoints/` directory.
Qwen3-VL-8B will be automatically downloaded from HuggingFace on first run.

### 3. Process Video

```bash
# Place video in input directory
cp your_video.mp4 data/input/

# Run processing
python run.py
```

### 4. Predefined Label Map (Optional)

You can specify predefined action labels in `data/input/label_map.txt`.
If this file exists and contains labels, only those labels will be used during recognition.

```
# label_map.txt example (both formats supported)

# Format 1: Labels only
standing
walking
running
punching
blocking

# Format 2: Number: Label
0: standing
1: walking
2: running
```

### 5. Check Results

**Intermediate Results** (`data/working/<video_name>/`):
- `tubes/` - Per-person cropped videos
- `tubes/metadata.json` - Tracking metadata
- `tubes/recognition_results.json` - Action recognition results
- `label_map.txt` - Action category mapping

**Final Output** (`data/output/`):
- `<video_name>.json` - Frame-level integrated label data

## âš™ï¸ Command Line Options

```bash
python run.py [OPTIONS]

Options:
  --skip-recognition          Skip action recognition phase (detection only)
  --refinement-iterations N   Number of refinement iterations (default: 5)
  --input-dir DIR             Input video directory (default: ./data/input)
  --working-dir DIR           Working directory (default: ./data/working)
  --output-dir DIR            Output directory (default: ./data/output)
```

**Examples:**

```bash
# Detection only
python run.py --skip-recognition

# 3 refinement iterations
python run.py --refinement-iterations 3

# Custom directories
python run.py --input-dir ./my_videos --output-dir ./my_output
```

## ğŸ“Š Output Format

### Final Label Data (`output/<video>.json`)

Integrated data containing frame-level bbox and action labels:

```json
{
  "version": "1.0",
  "video": {
    "name": "demo",
    "fps": 120.08,
    "width": 1920,
    "height": 1080,
    "total_frames": 1521,
    "duration": 12.67
  },
  "labelmap": {
    "0": {"name": "attacking", "original_actions": ["punching", ...]},
    "1": {"name": "boxing practice", "original_actions": [...]}
  },
  "num_persons": 3,
  "persons": {
    "id_1": {
      "action_summary": ["defending", "boxing practice"],
      "action_timeline": {
        "0": {"action": "standing", "action_id": 15}
      },
      "frames": [
        {
          "frame_idx": 0,
          "timestamp": 0.0,
          "bbox": {"x1": 1006, "y1": 157, "x2": 1275, "y2": 599},
          "action": "standing and moving",
          "action_id": 15
        }
      ]
    }
  },
  "summary": {
    "total_action_instances": 36,
    "action_distribution": {"boxing practice": 16, "defending": 3}
  }
}
```

### label_map.txt

Similar action grouping results:

```
0: attacking
   -> punching, throwing punch, raising arms
1: boxing practice
   -> boxing, sparring, shadow boxing
2: defending
   -> blocking, defending, preparing to block
```

### recognition_results.json (Intermediate)

```json
{
  "video_path": "./data/input/demo.mp4",
  "labelmap": ["attacking", "boxing practice", "defending", ...],
  "action_groups": {
    "boxing practice": ["boxing", "sparring", "shadow boxing"]
  },
  "tubes": {
    "id_1": {
      "temporal_labels": {
        "0": [
          {"action": "standing", "confidence": 0.90},
          {"action": "looking", "confidence": 0.75}
        ]
      },
      "final_actions": [
        {"time": 0, "action": "standing", "confidence": 0.90}
      ]
    }
  },
  "id_time_actions": [
    {"id": "id_1", "time": 0, "action": "standing", "confidence": 0.90}
  ]
}
```

## ğŸ”§ Configuration

### ByteTrack Parameters (`modules/bytetrack_tuned.yaml`)

| Parameter | Default | Description |
|-----------|---------|-------------|
| `track_high_thresh` | 0.5 | First-stage matching threshold |
| `track_low_thresh` | 0.1 | Second-stage low-confidence matching |
| `new_track_thresh` | 0.6 | New track creation threshold |
| `track_buffer` | 60 | Lost track retention frames |
| `match_thresh` | 0.8 | IoU-based association threshold |

## ğŸ“š Module Documentation

For detailed module documentation, see the `docs/` directory:

- [detector.md](detector.md) - Detection and tracking module
- [recognizer.md](recognizer.md) - Action recognition module
- [exporter.md](exporter.md) - Label data export module
- [dataset.md](dataset.md) - Dataset utilities
- [bytetrack_config.md](bytetrack_config.md) - ByteTrack configuration
- [logger.md](logger.md) - Logging utility

## ğŸ“¦ Dependencies

- Python >= 3.8
- PyTorch >= 2.1.0
- ultralytics >= 8.0.0
- transformers >= 4.38.0
- OpenCV
- Rich (logging)
- Qwen-VL-Utils

## ğŸ” Performance Considerations

- **GPU Memory**: Qwen3-VL-8B requires approximately 16GB VRAM
- **Processing Time**: ~5-7 minutes for 1-minute video (GPU, iterations=5)
- **Minimum Tube Length**: Tracks under 30 frames are filtered out

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- [Ultralytics YOLO](https://github.com/ultralytics/ultralytics)
- [ByteTrack](https://github.com/ifzhang/ByteTrack)
- [Qwen-VL](https://github.com/QwenLM/Qwen-VL)
